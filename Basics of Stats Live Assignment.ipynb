{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "1. Explain the different types of data (qualitative and quantitative) and provide examples of each. Discuss\nnominal, ordinal, interval, and ratio scales\n",
      "metadata": {
        "id": "M3rBWPxrDlrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": "Data can generally be classified into two main types: qualitative and quantitative. Here’s a breakdown of each type, along with the various scales of measurement.\n\nQualitative Data\nQualitative data describes characteristics or qualities that cannot be measured numerically. It’s often categorical and provides insights into the attributes of a subject.\n\nExamples:\n\nNominal Data: This is the simplest form of data, which can be categorized but not ranked. Examples include:\n\nTypes of fruits (apple, orange, banana)\nHair color (blonde, brown, black)\nOrdinal Data: This type involves categories that can be ordered or ranked but do not have a defined distance between them. Examples include:\n\nSurvey responses like satisfaction levels (satisfied, neutral, dissatisfied)\nEducation levels (high school, bachelor’s, master’s)\nQuantitative Data\nQuantitative data represents numerical values and can be measured or counted. This type of data can be further divided into two subcategories based on the scale of measurement.\n\nExamples:\n\nInterval Data: This type has numerical values with meaningful differences between them but lacks a true zero point. Examples include:\n\nTemperature in Celsius or Fahrenheit (20°C is not \"twice as hot\" as 10°C)\nDates (the difference between 2000 and 2010 is meaningful, but 0 does not indicate an absence of time)\nRatio Data: This type also has meaningful differences and includes a true zero point, allowing for the calculation of ratios. Examples include:\n\nHeight (0 cm means no height)\nWeight (0 kg means no weight)",
      "metadata": {
        "id": "1Gse_2hiDrLs"
      }
    },
    {
      "cell_type": "markdown",
      "source": "2.  What are the measures of central tendency, and when should you use each? Discuss the mean, median,\nand mode with examples and situations where each is appropriate",
      "metadata": {
        "id": "bFojmgEqDzBX"
      }
    },
    {
      "cell_type": "code",
      "source": "# 1. Mean\n# The mean is the average of a set of numbers. It is calculated by summing all values and dividing by the count of those values\n\ndata = [2, 4, 6, 8, 10]\nmean = sum(data) / len(data)\nprint(\"Mean:\", mean)\n\n# 2. Median\n# The median is the middle value of a data set when ordered. If there’s an even number of values, it’s the average of the two middle values\n\ndata_odd = [2, 4, 6, 8, 10]\ndata_even = [2, 4, 6, 8]\n\ndef calculate_median(data):\n    sorted_data = sorted(data)\n    n = len(sorted_data)\n    mid = n // 2\n    if n % 2 == 0:\n        return (sorted_data[mid - 1] + sorted_data[mid]) / 2\n    else:\n        return sorted_data[mid]\n\nmedian_odd = calculate_median(data_odd)\nmedian_even = calculate_median(data_even)\n\nprint(\"Median (odd):\", median_odd)\nprint(\"Median (even):\", median_even)\n\n# 3. Mode\n# The mode is the most frequently occurring value in a data set. A data set can have one mode, multiple modes (bimodal or multimodal), or no mode\n\nfrom collections import Counter\n\ndata1 = [1, 2, 2, 3, 4]\ndata2 = [1, 1, 2, 2, 3]\n\ndef calculate_mode(data):\n    count = Counter(data)\n    max_count = max(count.values())\n    modes = [key for key, value in count.items() if value == max_count]\n    return modes\n\nmode1 = calculate_mode(data1)\nmode2 = calculate_mode(data2)\n\nprint(\"Mode (single):\", mode1)\nprint(\"Mode (bimodal):\", mode2)",
      "metadata": {
        "id": "imSi7SIh1ZSj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "3. Explain the concept of dispersion. How do variance and standard deviation measure the spread of data?",
      "metadata": {
        "id": "zNQGn3QXFg5R"
      }
    },
    {
      "cell_type": "code",
      "source": "# Two common measures of dispersion are variance and standard deviation. Both quantify how much the values in a dataset deviate from the mean\n\n# 1. Variance\n# Variance measures the average of the squared differences between each data point and the mean. It provides an indication of how spread out the data points are.\n\ndata = [2, 4, 6, 8, 10]\nmean = sum(data) / len(data)\nvariance = sum((x - mean) ** 2 for x in data) / (len(data) - 1)\nprint(\"Variance:\", variance)\n\n# 2. Standard Deviation\n# Standard deviation is the square root of variance. It provides a measure of dispersion in the same units as the original data, making it more interpretable\n\nimport math\n\nstandard_deviation = math.sqrt(variance)\nprint(\"Standard Deviation:\", standard_deviation)",
      "metadata": {
        "id": "9tb8Jh2VFqjl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "4. What is a box plot, and what can it tell you about the distribution of data?\n\n",
      "metadata": {
        "id": "LATC9zoFGNMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": "A box plot (or box-and-whisker plot) is a graphical representation of a dataset that summarizes its key statistical features. It provides a visual overview of the distribution, central tendency, and variability of the data. Here’s a breakdown of its components and what it reveals about the data distribution.\n\nComponents of a Box Plot\nBox:\n\nThe central box represents the interquartile range (IQR), which contains the middle 50% of the data.\nThe bottom of the box is the first quartile (Q1) (25th percentile), and the top of the box is the third quartile (Q3) (75th percentile).\nMedian Line:\n\nA line inside the box indicates the median (the 50th percentile) of the dataset.\nWhiskers:\n\nLines extending from the box (the whiskers) represent the range of the data, typically extending to the smallest and largest values within 1.5 times the IQR from Q1 and Q3.\nValues outside this range are considered outliers.\nOutliers:\n\nPoints that fall beyond the whiskers are plotted individually, indicating values that are significantly lower or higher than the rest of the data.\nWhat a Box Plot Reveals\nCentral Tendency: The median line shows the center of the dataset.\nSpread of the Data: The width of the box indicates the IQR, reflecting the variability of the middle 50% of the data.\nSkewness: The position of the median line within the box (toward the top or bottom) can indicate skewness:\nIf the median is closer to Q1, the data may be right-skewed (positively skewed).\nIf it is closer to Q3, the data may be left-skewed (negatively skewed).\nOutliers: Individual points beyond the whiskers highlight outliers, which can be important for further analysis.\nComparative Analysis: Box plots are useful for comparing distributions between multiple groups, making them ideal for visualizing differences in datasets.\nExample of a Box Plot\nIf you have a dataset representing the scores of students in a class:\n\nData: [56, 67, 70, 75, 80, 82, 85, 88, 90, 95]\nBox Plot Representation: You would create a box plot with:\nQ1 at 70\nMedian at 80\nQ3 at 88\nWhiskers extending to the minimum and maximum values within 1.5 IQRs",
      "metadata": {
        "id": "kb7eWzegGYP5"
      }
    },
    {
      "cell_type": "markdown",
      "source": "5. Discuss the role of random sampling in making inferences about populations",
      "metadata": {
        "id": "nZqLZaOwGlRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": "Random sampling is a crucial method in statistics that allows researchers to make inferences about a larger population based on a smaller subset (sample) of that population. Here’s a discussion on its role and importance:\n\n1. Random sampling involves selecting individuals from a population in such a way that each individual has an equal chance of being chosen. This method helps eliminate biases that could distort the representation of the population\n\n2. Random sampling plays a key role in statistical inference, which is the process of drawing conclusions about a population based on sample data. Here are some of its key contributions\n\na) Random samples tend to be more representative of the population. By ensuring that every member has an equal chance of selection, researchers can avoid systematic biases that might skew the results\n\nb) This representativeness is essential for generalizing findings from the sample to the larger population\n\n3. While random sampling is powerful, it also presents challenges\n\na) Sample Size: The larger the sample, the more reliable the inferences. Small samples may not capture the diversity of the population.\n\nb) Practicality: In some cases, it may be difficult or costly to obtain a truly random sample, leading researchers to use convenience sampling or other methods that may introduce bias.\n\nc) on-response: Even with random sampling, if certain individuals do not respond, it can lead to bias if the non-respondents differ significantly from respondents.\n",
      "metadata": {
        "id": "-osJ9FfRG52B"
      }
    },
    {
      "cell_type": "markdown",
      "source": "6.  Explain the concept of skewness and its types. How does skewness affect the interpretation of data?",
      "metadata": {
        "id": "LzUtTPg7H1O6"
      }
    },
    {
      "cell_type": "markdown",
      "source": "Skewness is a statistical measure that describes the asymmetry of a probability distribution. It indicates the extent to which data points deviate from a normal distribution, which is symmetric. Understanding skewness is important for interpreting data accurately and for selecting appropriate statistical methods\n\n1. Positive Skewness (Right Skewness)\n\na) In a positively skewed distribution, the tail on the right side is longer or fatter than the left side. Most data points are concentrated on the left, with a few extreme values on the right\n\nb) Example: Income distribution in many societies, where most people earn below the average income, but a small number earn significantly more.\n\n2. Negative Skewness (Left Skewness)\n\na) In a negatively skewed distribution, the tail on the left side is longer or fatter than the right side. Most data points are concentrated on the right, with a few extreme values on the left\n\nb) Example: Age at retirement, where most people retire around the same age, but a few retire much earlier\n\n3. Zero Skewness (Symmetrical)\n\na) A distribution is considered symmetrical if it has no skewness, meaning that the left and right sides are mirror images. The normal distribution is an example of this\n\nHow Skewness Affects Interpretation of Data:-\n\nSkewness affects the relationship between the mean, median, and mode. In positively skewed data, the mean is pulled to the right, often giving a misleading representation of the \"typical\" value. Conversely, in negatively skewed data, the mean is pulled to the left\n\nMany statistical tests assume normality (zero skewness). If data is skewed, the assumptions of these tests may be violated, leading to unreliable results\n\nIn business and social sciences, understanding skewness helps in decision-making processes. For instance, if customer satisfaction scores are positively skewed, it may suggest that while most customers are satisfied, a small number are very dissatisfied",
      "metadata": {
        "id": "TErz2LbnH6FA"
      }
    },
    {
      "cell_type": "markdown",
      "source": "7.  What is the interquartile range (IQR), and how is it used to detect outliers?",
      "metadata": {
        "id": "xZVRYHa0JV9T"
      }
    },
    {
      "cell_type": "code",
      "source": "# The interquartile range (IQR) is a measure of statistical dispersion that quantifies the spread of the middle 50% of a dataset\n# It is particularly useful for detecting outliers. Here’s how to calculate the IQR and use it to identify outliers using Python\n\nimport numpy as np\n\ndata = [1, 3, 5, 7, 9, 11, 13, 15, 17]\n\nQ1 = np.percentile(data, 25)\nQ3 = np.percentile(data, 75)\n\nIQR = Q3 - Q1\n\nlower_boundary = Q1 - 1.5 * IQR\nupper_boundary = Q3 + 1.5 * IQR\n\noutliers = [x for x in data if x < lower_boundary or x > upper_boundary]\n\nprint(\"Q1:\", Q1)\nprint(\"Q3:\", Q3)\nprint(\"IQR:\", IQR)\nprint(\"Lower Boundary:\", lower_boundary)\nprint(\"Upper Boundary:\", upper_boundary)\nprint(\"Outliers:\", outliers)",
      "metadata": {
        "id": "QoRTrFb0KVxn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "8. Discuss the conditions under which the binomial distribution is used",
      "metadata": {
        "id": "BcG0psztKshH"
      }
    },
    {
      "cell_type": "code",
      "source": "#The binomial distribution is a discrete probability distribution that describes the number of\n#successes in a fixed number of independent Bernoulli trials, each with the same probability of success\n\nimport numpy as np\nfrom scipy.stats import binom\nimport matplotlib.pyplot as plt\n\n# Parameters\nn = 10  # number of trials\np = 0.5  # probability of success\nk = 4    # number of successes\n\n# Calculate the probability of getting exactly k successes\nprobability_k_successes = binom.pmf(k, n, p)\nprint(f\"Probability of getting exactly {k} successes in {n} trials: {probability_k_successes:.4f}\")\n\n# Calculate cumulative probability of getting at most k successes\ncumulative_prob = binom.cdf(k, n, p)\nprint(f\"Cumulative probability of getting at most {k} successes: {cumulative_prob:.4f}\")\n\nx = np.arange(0, n + 1)\nbinomial_pmf = binom.pmf(x, n, p)\n\nplt.bar(x, binomial_pmf)\nplt.title(f'Binomial Distribution PMF (n={n}, p={p})')\nplt.xlabel('Number of Successes')\nplt.ylabel('Probability')\nplt.xticks(x)\nplt.show()",
      "metadata": {
        "id": "XIPbkVFELS1v"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "9.  Explain the properties of the normal distribution and the empirical rule (68-95-99.7 rule).",
      "metadata": {
        "id": "K6d97szyLnZa"
      }
    },
    {
      "cell_type": "code",
      "source": "# The normal distribution is a key concept in statistics, characterized by its symmetry, defined by mean and standard deviation, and described by the empirical rule\n# The Python code demonstrates how to visualize these properties effectively, providing a clearer understanding of the distribution and the empirical rule\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n\nmu = 0\nsigma = 1\n\nx = np.linspace(mu - 4*sigma, mu + 4*sigma, 1000)\ny = stats.norm.pdf(x, mu, sigma)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x, y, label='Normal Distribution', color='blue')\n\nplt.fill_between(x, y, where=(x >= mu - sigma) & (x <= mu + sigma), color='lightblue', alpha=0.5, label='68%')\nplt.fill_between(x, y, where=(x >= mu - 2*sigma) & (x <= mu + 2*sigma), color='lightgreen', alpha=0.5, label='95%')\nplt.fill_between(x, y, where=(x >= mu - 3*sigma) & (x <= mu + 3*sigma), color='lightcoral', alpha=0.5, label='99.7%')\n\nplt.title('Normal Distribution with Empirical Rule')\nplt.xlabel('Value')\nplt.ylabel('Probability Density')\nplt.axvline(mu, color='black', linestyle='--', label='Mean (μ)')\nplt.legend()\nplt.grid()\nplt.show()",
      "metadata": {
        "id": "mzbJk1YxMrGl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "10.  Provide a real-life example of a Poisson process and calculate the probability for a specific event",
      "metadata": {
        "id": "ijJvvfPjM_3Q"
      }
    },
    {
      "cell_type": "code",
      "source": "# A classic example of a Poisson process is the number of customers arriving at a bank during a specific hour\n#  Assume that, on average, 3 customers arrive at the bank every hour\n\nimport math\n\nlambda_value = 3\nk = 5\n\nprobability = (math.exp(-lambda_value) * (lambda_value ** k)) / math.factorial(k)\n\nprint(f\"Probability of exactly {k} customers arriving: {probability:.4f}\")",
      "metadata": {
        "id": "-yGbZeCMOOVU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "11.  Explain what a random variable is and differentiate between discrete and continuous random variables",
      "metadata": {
        "id": "ba1GwPjxQ4_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": "A random variable is a numerical outcome of a random phenomenon. It is a function that assigns a real number to each possible outcome of a random experiment. Random variables are fundamental in statistics and probability theory because they allow us to quantify and analyze the variability of outcomes\n\n1. Discrete Random Variables\n\nA discrete random variable can take on a countable number of distinct values. These values are often integers, and each value can be associated with a specific probability. Discrete random variables are used to represent scenarios where the outcomes can be enumerated\n\n2. Continuous Random Variables\n\nA continuous random variable can take on an infinite number of possible values within a given range. These values are not countable, and the random variable is often associated with measurements. Continuous random variables represent scenarios where outcomes can vary smoothly.",
      "metadata": {
        "id": "JTp-FaSaRLvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": "12. Provide an example dataset, calculate both covariance and correlation, and interpret the results",
      "metadata": {
        "id": "PIO4p9MoRaFh"
      }
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\n\ndata = {\n    'Hours_Studied': [2, 3, 4, 5, 6, 7],\n    'Exam_Score': [65, 70, 75, 80, 85, 90]\n}\n\ndf = pd.DataFrame(data)\n\ncovariance = np.cov(df['Hours_Studied'], df['Exam_Score'])[0][1]\n\ncorrelation = np.corrcoef(df['Hours_Studied'], df['Exam_Score'])[0][1]\n\nprint(f\"Covariance: {covariance:.2f}\")\nprint(f\"Correlation: {correlation:.2f}\")",
      "metadata": {
        "id": "OGgH7suWRdnQ"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}